{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5VNNlvBASKZ",
        "outputId": "948d2425-1d3a-462e-d1f6-84dc748c89cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/학부연구생\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/학부연구생"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgsNN7o6rFRm",
        "outputId": "b0fde025-8852-46d9-d650-1d6539e13e2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'SimCTG'...\n",
            "remote: Enumerating objects: 2380, done.\u001b[K\n",
            "remote: Counting objects: 100% (745/745), done.\u001b[K\n",
            "remote: Compressing objects: 100% (148/148), done.\u001b[K\n",
            "remote: Total 2380 (delta 711), reused 596 (delta 596), pack-reused 1635\u001b[K\n",
            "Receiving objects: 100% (2380/2380), 6.79 MiB | 9.49 MiB/s, done.\n",
            "Resolving deltas: 100% (1486/1486), done.\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/yxuansu/SimCTG.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Wev7_BVrP0i"
      },
      "outputs": [],
      "source": [
        "# !chmod +x /content/SimCTG/data/download_wikipedia.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ni5tumNruv7",
        "outputId": "e8407b40-2db6-4f0c-d1ed-09ea6a38b6c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/1/uc?export=download&confirm=5_7Y&id=1GSL7FNr_pJAu7T_6VL9HwedTBvi-zZ6N\n",
            "To: /content/wikipedia.zip\n",
            "100% 5.41G/5.41G [01:00<00:00, 89.7MB/s]\n",
            "Archive:  wikipedia.zip\n",
            "   creating: wikipedia/\n",
            "  inflating: wikipedia/dev_english_wikipedia.txt  \n",
            "  inflating: __MACOSX/wikipedia/._dev_english_wikipedia.txt  \n",
            "  inflating: wikipedia/train_english_wikipedia.txt  \n"
          ]
        }
      ],
      "source": [
        "# !/content/SimCTG/data/download_wikipedia.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvxYbOzWCH8X",
        "outputId": "b4888c0c-a4ad-4751-9990-47cf716331a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install transformers>=4.24.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9mBvckIsF8R",
        "outputId": "8513fc6f-ac68-4a3d-93c4-576d249d4a36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda is available.\n",
            "Using single GPU training.\n",
            "Loading dataset...\n",
            "tokenizer loaded\n",
            "Loading dev data...\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (235696 > 512). Running this sequence through the model will result in indexing errors\n",
            "Number of dev batches is 453\n",
            "Dev data loaded.\n",
            "Dataset loaded.\n",
            "Initializing model...\n",
            "True False\n",
            "Further pre-train with available parameters.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Model loaded\n",
            "total training steps is 350, warmup steps is 105\n",
            "--------------------------------------------------------------------------\n",
            "Start Pre-training:\n",
            "At training steps 50, training MLE loss is 6.4590421903133395, train CL loss is 0.9170483484864235\n",
            "At training steps 100, training MLE loss is 3.5209665540605783, train CL loss is 0.4701987395973993\n",
            "At training steps 150, training MLE loss is 2.4386863555014133, train CL loss is 0.31353701834314657\n",
            "At training steps 200, training MLE loss is 1.9126103917136787, train CL loss is 0.2354974281998875\n",
            "At training steps 250, training MLE loss is 1.5762581959813833, train CL loss is 0.18856613765086513\n",
            "At training steps 300, training MLE loss is 1.354024373764793, train CL loss is 0.1571539764852302\n",
            "At training steps 350, training MLE loss is 1.193363961971232, train CL loss is 0.1348112584554058\n",
            "| |                                #                                     | 452 Elapsed Time: 0:00:17\n",
            "At training steps 350, training MLE loss is 1.193363961971232, train CL loss is 0.1348112584554058, validation ppl is 175.70170522209156\n",
            "Saving model...\n",
            "Model Saved!\n",
            "-----------------------------------\n",
            "Pre-training Completed!\n",
            "--------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "!chmod +x /content/drive/MyDrive/학부연구생/SimCTG/pretraining/train.sh\n",
        "!/content/drive/MyDrive/학부연구생/SimCTG/pretraining/train.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx3DgbkA80Hm"
      },
      "source": [
        "# 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "Z_Vu6jFS2fT3",
        "outputId": "e18a860a-044a-4680-a639-b64fd7a5a3bc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f0c79188-04d1-4dd9-b881-a6a75081cef5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f0c79188-04d1-4dd9-b881-a6a75081cef5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving opposite_sent_RG_refined.txt to opposite_sent_RG_refined.txt\n",
            "File saved as output.xlsx\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_e38db569-87fc-4ad5-9f37-ed5c0fcc93ea\", \"output.xlsx\", 793220)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# 코랩 환경에서 파일 업로드\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "    input_path = fn\n",
        "\n",
        "# txt 파일을 DataFrame으로 읽어오기\n",
        "df = pd.read_csv(input_path, delimiter='\\t', header=None, names=['ID', 'Origin Stereotype', 'Opposite Stereotype'])\n",
        "\n",
        "# 엑셀 파일로 저장\n",
        "output_filename = \"output.xlsx\"\n",
        "df.to_excel(output_filename, index=False)\n",
        "print(f\"File saved as {output_filename}\")\n",
        "\n",
        "# 코랩 환경에서 엑셀 파일 다운로드\n",
        "files.download(output_filename)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "89549300e1bd4f6daef159915050c6f4",
            "09a21f85e60a42ce942848c0fde73aa4",
            "adc8b9eaf89642a8894e8845b1f697d4",
            "a9f71a49602a4b348a3523208381dd49",
            "f6156c76bea74995bd4d4d14b52454fa",
            "7c06c67dcc8b4cb6ada5c2364e6a2c0b",
            "194dc3369d374a718978e861e6503dcd"
          ]
        },
        "id": "oBu9iN2si1fN",
        "outputId": "dd80cc5a-ef8c-405d-b614-47ed3f7bcd9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Using cached datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Using cached multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.14.6 dill-0.3.7 multiprocess-0.70.15\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89549300e1bd4f6daef159915050c6f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/4.26k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09a21f85e60a42ce942848c0fde73aa4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading metadata:   0%|          | 0.00/2.90k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adc8b9eaf89642a8894e8845b1f697d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/17.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9f71a49602a4b348a3523208381dd49",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/9.46M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6156c76bea74995bd4d4d14b52454fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/17501 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c06c67dcc8b4cb6ada5c2364e6a2c0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/16738 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "194dc3369d374a718978e861e6503dcd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/112900 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install datasets\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "\n",
        "bias_frame = load_dataset('social_bias_frames')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY1uAgqijNkk"
      },
      "outputs": [],
      "source": [
        "# anchor_plus 와 연결하기 위해 작업 데이터셋 로드\n",
        "import pandas as pd\n",
        "excel_file_path = '/content/drive/MyDrive/학부연구생/SimCTG_code/prof_test.xlsx'\n",
        "df_my = pd.read_excel(excel_file_path)\n",
        "\n",
        "# 리스트에 3컬럼 각각 담기.\n",
        "anchor_list = []\n",
        "anchor_minus_list = []\n",
        "anchor_plus_list = []\n",
        "ID_list = []\n",
        "\n",
        "for i in range(112900):\n",
        "  anchor_minus = bias_frame['train'][i]['targetStereotype']\n",
        "  if anchor_minus !='':\n",
        "    anchor = bias_frame['train'][i]['post']\n",
        "    anchor_plus = df_my[df_my['Origin Stereotype'] == anchor_minus]['Opposite Stereotype']\n",
        "    # anchor_minus -> anchor -> anchor_plus 를 뽑고, anchor_minus 에서 anchor_plus 를 뽑아서 중복발생.. 크게 문제는 없음!\n",
        "    if not anchor_plus.empty:\n",
        "      ID_list.append(i)\n",
        "      anchor_plus_list.append(anchor_plus.values[0])\n",
        "      anchor_list.append(anchor)\n",
        "      anchor_minus_list.append(anchor_minus)\n",
        "\n",
        "cleaned_anchor_list = []\n",
        "for anchor_text in anchor_list:\n",
        "    # RT, @[단어], $#숫자 등의 패턴을 제거\n",
        "    cleaned_text = re.sub(r'RT\\s*@[^: ]*:|@\\w+|\\$\\d+', '', anchor_text)\n",
        "    # #[단어] 패턴을 제거\n",
        "    cleaned_text = re.sub(r'#\\w+', '', cleaned_text)\n",
        "    # URL 제거\n",
        "    cleaned_text = re.sub(r'http\\S+', '', cleaned_text)\n",
        "    # '&'와 ';' 그리고 ':' 제거\n",
        "    cleaned_text = re.sub(r'[&;:]', '', cleaned_text)\n",
        "    # 공백 문자 여러 개를 하나로 줄임\n",
        "    cleaned_text = ' '.join(cleaned_text.split())\n",
        "\n",
        "    cleaned_anchor_list.append(cleaned_text)\n",
        "\n",
        "data = {'ID': ID_list,\n",
        "        'anchor': cleaned_anchor_list,\n",
        "        'anchor_minus': anchor_minus_list,\n",
        "        'anchor_plus': anchor_plus_list}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df.to_csv('prof_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HED2tx48fKr0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "cleaned_anchor_list = []\n",
        "for anchor_text in anchor_list:\n",
        "    # RT, @[단어], $#숫자 등의 패턴을 제거\n",
        "    cleaned_text = re.sub(r'RT\\s*@[^: ]*:|@\\w+|\\$\\d+', '', anchor_text)\n",
        "    # #[단어] 패턴을 제거\n",
        "    cleaned_text = re.sub(r'#\\w+', '', cleaned_text)\n",
        "    # URL 제거\n",
        "    cleaned_text = re.sub(r'http\\S+', '', cleaned_text)\n",
        "    # '&'와 ';' 그리고 ':' 제거\n",
        "    cleaned_text = re.sub(r'[&;:]', '', cleaned_text)\n",
        "    # 공백 문자 여러 개를 하나로 줄임\n",
        "    cleaned_text = ' '.join(cleaned_text.split())\n",
        "\n",
        "    cleaned_anchor_list.append(cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5j1qVSlmOa-"
      },
      "source": [
        "# Generation\n",
        "\n",
        "- GPT 계열 모델을 head 로 해야 함."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj_9wDjc3qTQ",
        "outputId": "fb2d4fc5-bd34-48db-84c5-da46dfba56d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/학부연구생/SimCTG\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/학부연구생/SimCTG/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOvCHuHq3dJb",
        "outputId": "e584f14a-93ee-4aef-9958-116a3dfaf15a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (7.4.2)\n",
            "Collecting sacrebleu==1.4.10 (from -r requirements.txt (line 3))\n",
            "  Downloading sacrebleu-1.4.10-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.41.2)\n",
            "Collecting progressbar (from -r requirements.txt (line 6))\n",
            "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.15.2+cu118)\n",
            "Collecting transformers (from -r requirements.txt (line 10))\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (6.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (3.8.1)\n",
            "Collecting sentencepiece (from -r requirements.txt (line 13))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (3.6.1)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (4.6.6)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (0.12.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (1.5.3)\n",
            "Collecting portalocker (from sacrebleu==1.4.10->-r requirements.txt (line 3))\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->-r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->-r requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->-r requirements.txt (line 2)) (1.1.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->-r requirements.txt (line 2)) (2.0.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 8)) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 8)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 8)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 8)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 8)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 8)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 8)) (3.27.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 8)) (17.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 9)) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 9)) (9.4.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers->-r requirements.txt (line 10))\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 10)) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers->-r requirements.txt (line 10))\n",
            "  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 10))\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 10)) (4.66.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 12)) (8.1.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 14)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 14)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 14)) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 14)) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 14)) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 14)) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 14)) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 14)) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 14)) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 14)) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 14)) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 14)) (6.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 14)) (1.10.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 14)) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 14)) (3.3.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 15)) (4.11.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 17)) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 17)) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 17)) (4.43.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 17)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 17)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 17)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 18)) (2023.3.post1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers->-r requirements.txt (line 10)) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 9)) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 9)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 9)) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 9)) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->-r requirements.txt (line 14)) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->-r requirements.txt (line 14)) (0.1.3)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers->-r requirements.txt (line 10))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->-r requirements.txt (line 15)) (2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 8)) (2.1.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 9)) (1.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 8)) (1.3.0)\n",
            "Building wheels for collected packages: progressbar\n",
            "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12067 sha256=5dfc7714b181535463572b621806a936a14fe475abe510992c93ff303dbe9cef\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/17/e5/765d1a3112ff3978f70223502f6047e06c43a24d7c5f8ff95b\n",
            "Successfully built progressbar\n",
            "Installing collected packages: sentencepiece, safetensors, progressbar, portalocker, sacrebleu, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 portalocker-2.8.2 progressbar-2.5 sacrebleu-1.4.10 safetensors-0.3.3 sentencepiece-0.1.99 tokenizers-0.14.0 transformers-4.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBBb-CM-nJ3X",
        "outputId": "5af8a8da-9513-4c86-da44-c45d2f8c6824"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting simctg\n",
            "  Using cached simctg-0.7.tar.gz (8.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from simctg) (1.4.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from simctg) (7.4.1)\n",
            "Collecting sacrebleu==1.4.10 (from simctg)\n",
            "  Using cached sacrebleu-1.4.10-py3-none-any.whl (60 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from simctg) (1.16.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from simctg) (0.41.2)\n",
            "Collecting progressbar (from simctg)\n",
            "  Using cached progressbar-2.5.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sklearn (from simctg)\n",
            "  Using cached sklearn-0.0.post9.tar.gz (3.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install simctg --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q50QlGxt42A6",
        "outputId": "6bc59e00-f1b9-4870-c8cf-6c6211b6949a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/학부연구생/SimCTG/pretraining\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/학부연구생/SimCTG/pretraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrIFIY3U539G"
      },
      "outputs": [],
      "source": [
        "import simctg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-5gjX1RmQPl",
        "outputId": "e2e6fef6-5c28-4065-9208-3eb30065d42a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Further pre-train with available parameters.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type gpt2 to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at /content/drive/MyDrive/학부연구생/SimCTG_ver4/training_step_50_train_mle_loss_3.431_train_cl_loss_0.512_dev_ppl_199.648 and are newly initialized: ['encoder.layer.10.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.6.attention.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.10.attention.self.key.bias', 'lm_head.layer_norm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'lm_head.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'lm_head.layer_norm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'lm_head.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'lm_head.decoder.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'lm_head.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'lm_head.decoder.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "SimCTGPretraining(\n",
              "  (model): RobertaForMaskedLM(\n",
              "    (roberta): RobertaModel(\n",
              "      (embeddings): RobertaEmbeddings(\n",
              "        (word_embeddings): Embedding(50257, 768, padding_idx=1)\n",
              "        (position_embeddings): Embedding(512, 768, padding_idx=1)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): RobertaEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-11): 12 x RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (lm_head): RobertaLMHead(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (decoder): Linear(in_features=768, out_features=50257, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (logsftmax): LogSoftmax(dim=-1)\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from simctg_my import SimCTGPretraining\n",
        "model_path = r'/content/drive/MyDrive/학부연구생/SimCTG_ver4/training_step_50_train_mle_loss_3.431_train_cl_loss_0.512_dev_ppl_199.648'\n",
        "model = SimCTGPretraining(model_path)\n",
        "model.eval()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "Fmy07GRtFudt",
        "outputId": "68e4a9a0-fa98-4ab3-ad1a-a3af08130309"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96b841952a924b3ab01a10113dbc2285",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/218 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "974bd488d9e9462cb12dcfa2b2276f91",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c7ef6d6662c405db97502092ff5c650",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41c4dc2df3f24567a86e0da12e18bbff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5abd852e959441987805dd802d3e895",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8a2d6f965834e32a72ecc0f69bcb8f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Further pre-train with available parameters.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7399ce33862541319f948523568312cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "SimCTGPretraining(\n",
              "  (model): GPT2LMHeadModel(\n",
              "    (transformer): GPT2Model(\n",
              "      (wte): Embedding(50257, 768)\n",
              "      (wpe): Embedding(1024, 768)\n",
              "      (drop): Dropout(p=0.1, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0-11): 12 x GPT2Block(\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              "  )\n",
              "  (logsftmax): LogSoftmax(dim=-1)\n",
              ")"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from simctg_my import SimCTGPretraining\n",
        "model_path = r'cambridgeltl/simctg_english_wikipedia'\n",
        "model_2 = SimCTGPretraining(model_path)\n",
        "model_2.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ea1Vq9WDg_R"
      },
      "outputs": [],
      "source": [
        "# Bitches really be mad when you get attention from a guy she likes amp it's even funnier when your not even trying to get his attention ‍♀️\n",
        "# prepare text prefix\n",
        "text = r'Black people deserves to be'\n",
        "# prepare text prefix\n",
        "tokens = model.tokenizer.tokenize(text)\n",
        "input_ids = model.tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = torch.LongTensor(input_ids).view(1,-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jo83Gf9msXm"
      },
      "outputs": [],
      "source": [
        "# use contrastive search to generate the result\n",
        "beam_width, alpha, decoding_len = 5, 0.6, 128\n",
        "eos_token = '<|endoftext|>'\n",
        "print(model.fast_contrastive_search(input_ids, beam_width, alpha, decoding_len, eos_token))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIyJ6NaZxw4A"
      },
      "source": [
        "# Wikitext 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8UujdIDx2Cq",
        "outputId": "1e67cc4b-2b75-4b79-dc0c-63027d2d5104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/학부연구생/SimCTG/data\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/학부연구생/SimCTG/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q19R_blIxylF",
        "outputId": "b3a73784-daa6-4cd6-8954-7752e697a1d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/1/uc?export=download&confirm=opP0&id=1yvGE0x4d6sqiQHsihzIM6MiCC36MV3lO\n",
            "To: /content/drive/MyDrive/학부연구생/SimCTG/data/language_modelling.zip\n",
            "100% 189M/189M [00:06<00:00, 27.8MB/s]\n",
            "Archive:  language_modelling.zip\n",
            "   creating: language_modelling/\n",
            "  inflating: language_modelling/.DS_Store  \n",
            "  inflating: __MACOSX/language_modelling/._.DS_Store  \n",
            "   creating: language_modelling/wikitext103/\n",
            "  inflating: language_modelling/wikitext103/wikitext103_raw_v1_test.txt  \n",
            "  inflating: language_modelling/wikitext103/.DS_Store  \n",
            "  inflating: language_modelling/wikitext103/wikitext103_raw_v1_validation.txt  \n",
            "   creating: language_modelling/wikitext103/.ipynb_checkpoints/\n",
            "  inflating: language_modelling/wikitext103/wikitext103_raw_v1_train.txt  \n",
            "  inflating: language_modelling/wikitext103/.ipynb_checkpoints/wiki103 dataset-checkpoint.ipynb  \n",
            "  inflating: language_modelling/wikitext103/.ipynb_checkpoints/Prepare Dataset-checkpoint.ipynb  \n"
          ]
        }
      ],
      "source": [
        "!chmod +x ./download_language_modelling_data.sh\n",
        "!./download_language_modelling_data.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbHLxVSfx0M3"
      },
      "outputs": [],
      "source": [
        "# /content/drive/MyDrive/학부연구생/SimCTG/data/language_modelling/wikitext103/wikitext103_raw_v1_validation.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VobmzHzoRp-o"
      },
      "source": [
        "# benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuHcyyAaRo_c",
        "outputId": "7f08e54f-beb7-4c1b-fbdf-666c7294fa1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/학부연구생\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/학부연구생"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EOBpLjtRuhQ",
        "outputId": "b9a81e9d-6852-4025-906f-e4b0b2f63fa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'MABEL'...\n",
            "remote: Enumerating objects: 213, done.\u001b[K\n",
            "remote: Counting objects: 100% (213/213), done.\u001b[K\n",
            "remote: Compressing objects: 100% (155/155), done.\u001b[K\n",
            "remote: Total 213 (delta 97), reused 157 (delta 56), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (213/213), 6.06 MiB | 4.77 MiB/s, done.\n",
            "Resolving deltas: 100% (97/97), done.\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/princeton-nlp/MABEL.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yt4zN8uiSBWq",
        "outputId": "1278fbc7-1f4f-4faa-abb8-c4e0e85d2884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/학부연구생/MABEL\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/학부연구생/MABEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgsCW8RrR0oQ"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWeIE586R_-l"
      },
      "outputs": [],
      "source": [
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7yQlAiGShPn",
        "outputId": "8c1aa1a3-abca-4668-fad7-b9f48ee4eae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install setuptools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uONlNF8aWouT"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQlq9XzrUFzC",
        "outputId": "0e2bcaa9-3ee0-42a8-80c4-8bcb8a760487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running StereoSet:\n",
            " - persistent_dir: /content/drive/MyDrive/학부연구생/MABEL/benchmark/intrinsic\n",
            " - model: BertForMaskedLM\n",
            " - model_name_or_path: /content/drive/MyDrive/학부연구생/bert_base_plus_lr14_seed74_st350_wm03/training_step_350_train_mle_loss_1.193_train_cl_loss_0.135_dev_ppl_175.702\n",
            " - batch_size: 1\n",
            " - seed: 26\n",
            "Evaluating intrasentence task.\n",
            "2023-11-06 10:49:17.129269: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-06 10:49:17.129325: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-06 10:49:17.129362: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-06 10:49:18.560338: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "100% 24464/24464 [06:27<00:00, 63.17it/s]\n"
          ]
        }
      ],
      "source": [
        "!python -m benchmark.intrinsic.stereoset.predict --seed 26 \\\n",
        "--model BertForMaskedLM \\\n",
        "--model_name_or_path /content/drive/MyDrive/학부연구생/bert_base_plus_lr14_seed74_st350_wm03/training_step_350_train_mle_loss_1.193_train_cl_loss_0.135_dev_ppl_175.702"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gs1evMmj7Iq",
        "outputId": "526ff468-83b0-4331-c880-0bce5b590ec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating StereoSet files:\n",
            " - predictions_file: /content/drive/MyDrive/학부연구생/MABEL/benchmark/intrinsic/stereoset/bert_base_plus_lr14_seed74_st350_wm03.json\n",
            " - predictions_dir: None\n",
            " - output_file: None\n",
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 2313.0\n",
            "\t\tLM Score: 80.78109270142005\n",
            "\t\tSS Score: 58.73306549540932\n",
            "\t\tICAT Score: 66.67176123437538\n"
          ]
        }
      ],
      "source": [
        "!python -m benchmark.intrinsic.stereoset.eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvdrUAa1pZcP"
      },
      "source": [
        "- 훈련 100 step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUyd46jKo7mF"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k9oUmzdo59h"
      },
      "source": [
        "- 훈련 진행 후"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEkBu6c3k40a"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYumG0bKpoT2"
      },
      "source": [
        "- 참고로 roberta-base 의 경우 88, 66, 59"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IMGtI0gYhGn",
        "outputId": "96291221-0416-465d-feee-10b06f12ccc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running CrowS-Pairs benchmark:\n",
            " - model: BertForMaskedLM\n",
            " - model_name_or_path: /content/drive/MyDrive/학부연구생/plus_lr45_seed74/training_step_422_train_mle_loss_0.678_train_cl_loss_0.098_dev_ppl_14.996\n",
            " - bias_type: gender\n",
            "Evaluating gender examples.\n",
            " 94% 245/262 [00:16<00:00, 21.49it/s]Skipping example 245.\n",
            "100% 261/262 [00:18<00:00, 14.28it/s]\n",
            "====================================================================================================\n",
            "Total examples: 261\n",
            "Metric score: 59.0\n",
            "Stereotype score: 66.04\n",
            "Anti-stereotype score: 48.04\n",
            "Num. neutral: 0.0\n",
            "====================================================================================================\n",
            "\n",
            "Metric: 59.0\n"
          ]
        }
      ],
      "source": [
        "!python -m benchmark.intrinsic.crows.eval --model_name_or_path /content/drive/MyDrive/학부연구생/plus_lr45_seed74/training_step_422_train_mle_loss_0.678_train_cl_loss_0.098_dev_ppl_14.996"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtBQpW9YYuVe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu3jO25d9dT3"
      },
      "source": [
        "# Hate speech Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CIuiaBF9fPl",
        "outputId": "fb86b02a-7a0c-4f60-cf4c-0c690411a517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.14.6 dill-0.3.7 multiprocess-0.70.15\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.23.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install accelerate\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "_PglcTe09idU",
        "outputId": "1f50eac4-b390-4b9e-9d46-63137904a291"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1206' max='1206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1206/1206 14:50, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.383800</td>\n",
              "      <td>0.361085</td>\n",
              "      <td>0.883489</td>\n",
              "      <td>0.828837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.342900</td>\n",
              "      <td>0.382570</td>\n",
              "      <td>0.883489</td>\n",
              "      <td>0.828837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.330200</td>\n",
              "      <td>0.400904</td>\n",
              "      <td>0.883489</td>\n",
              "      <td>0.828837</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='168' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [168/168 00:35]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.8789689951438177\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, load_metric\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# 1. 데이터 로드 코드\n",
        "dataset = load_dataset(\"hate_speech18\")\n",
        "\n",
        "# 0과 1이 아닌 라벨 데이터를 제외하는 함수\n",
        "def filter_labels(example):\n",
        "    return example['label'] in [0, 1]\n",
        "\n",
        "# 0과 1이 아닌 라벨을 제외하고 데이터를 필터링\n",
        "dataset = dataset.filter(filter_labels)\n",
        "\n",
        "\n",
        "# 저장된 모델 및 토크나이저의 경로 지정\n",
        "#MODEL_PATH = \"/content/drive/MyDrive/학부연구생/plus_margin_09/training_step_422_train_mle_loss_0.931_train_cl_loss_0.119_dev_ppl_16.268\"\n",
        "#TOKENIZER_PATH = \"/content/drive/MyDrive/학부연구생/plus_margin_09/training_step_422_train_mle_loss_0.931_train_cl_loss_0.119_dev_ppl_16.268\"\n",
        "MODEL_PATH = \"roberta-base\"\n",
        "TOKENIZER_PATH = \"roberta-base\"\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(TOKENIZER_PATH)\n",
        "model = RobertaForSequenceClassification.from_pretrained(MODEL_PATH, num_labels = 2)\n",
        "\n",
        "def tokenizer_function(example):\n",
        "  encoding = tokenizer(example['text'], truncation=True, padding='max_length', max_length=256)\n",
        "  return {\n",
        "      \"input_ids\": encoding['input_ids'],\n",
        "      \"attention_mask\": encoding[\"attention_mask\"],\n",
        "      \"label\": example[\"label\"]\n",
        "  }\n",
        "\n",
        "# 토큰화 수행\n",
        "train_val_dataset = dataset['train'].map(tokenizer_function, batched=True)\n",
        "\n",
        "# 2. train, val, test 세트로 분할\n",
        "\n",
        "train_size = int(0.6 * len(train_val_dataset))\n",
        "val_size = int(0.15 * len(train_val_dataset))\n",
        "test_size = len(train_val_dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(train_val_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# 3. 모델 및 토크나이저 로드 코드 (구글 드라이브 주소에서)\n",
        "\n",
        "# 4. 에폭마다 training loss, epoch, step, validation loss, accuracy 평가\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "def compute_metrics(p):\n",
        "  labels= p.label_ids\n",
        "  preds = p.predictions.argmax(-1)\n",
        "  f1 = f1_score(labels, preds, average = 'weighted')\n",
        "  acc = accuracy_score(preds, labels)\n",
        "  return {\"accuracy\": acc, \"f1\": f1}\n",
        "    #metric = load_metric(\"accuracy\")\n",
        "    #return metric.compute(predictions=p.predictions.argmax(-1), references=p.label_ids)\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/학부연구생/HSD_result/roberta-ep-3\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"/content/drive/MyDrive/학부연구생/HSD_result/roberta-ep-3\",\n",
        "    logging_steps=10,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    no_cuda=False,\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# 훈련 시작\n",
        "trainer.train()\n",
        "\n",
        "# test 세트에 대한 성능 측정\n",
        "test_result = trainer.evaluate(test_dataset)\n",
        "\n",
        "print(f\"Test Accuracy: {test_result['eval_accuracy']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1ORZlTwG-om"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Hx3DgbkA80Hm",
        "o5j1qVSlmOa-",
        "QIyJ6NaZxw4A"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}